# -*- coding: utf-8 -*-
"""
Created on Thu Sep 15 20:04:54 2022

@author: Pratz
"""


import requests
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://www.airbnb.co.in/s/Honolulu--HI--United-States/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&flexible_trip_lengths%5B%5D=one_week&price_filter_input_type=0&query=Honolulu%2C%20HI&place_id=ChIJTUbDjDsYAHwRbJen81_1KEs&date_picker_type=calendar&checkin=2022-09-15&checkout=2022-09-24&source=structured_search_input_header&search_type=autocomplete_click'

page = requests.get(url)
page

soup = BeautifulSoup(page.text, 'lxml')
soup

df = pd.DataFrame({'Links':[''], 'Title':[''], 'Details':[''], 'Price':[''], 'Rating':['']})


while True:
    
    postings = soup.find_all('div',class_ = 'c4mnd7m dir dir-ltr')
    for post in postings:
        try:
            link = post.find('a', class_ = 'ln2bl2p dir dir-ltr').get('href')
            link_full = 'https://www.airbnb.co.in/' + link
            title = post.find('div', class_ = 't1jojoys dir dir-ltr').text
            price = post.find('span', class_ = 'a8jt5op dir dir-ltr').text
            ratings = post.find('span', class_ = 'ru0q88m dir dir-ltr').text[0:3]
            details = post.find_all('div', class_ = 'n1v28t5c s1cjsi4j dir dir-ltr')[0].text
            df = df.append({'Links':link_full, 'Title':title, 'Details':details, 'Price':price, 'Rating':ratings}, ignore_index = True)
        
        except:
            pass
        
    #below code goes to all the pages present one by one    
    
    next_page = soup.find('a', {'aria-label':'Next'}).get('href')
    next_page_full = 'https://www.airbnb.co.in/' + next_page
    next_page_full
        
    url = next_page_full
    page = requests.get(url)
    soup = BeautifulSoup(page.text, 'lxml')
    
df.to_csv(r'path u want/bnb_scrapped.csv') 
